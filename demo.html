<!doctype html>
<html>

<head>
  <meta charset="UTF-8">
  <title>WeBAD Demo</title>
  <link rel="shortcut icon" href="#">
  <style>
    body {
      font-size: 18px;
      font-family: 'courier';
      /*"Lucida Console", Courier, monospace;*/
      background-color: black;
      color: white;
    }

    button {
      font-size: 18px;
      font-family: 'courier';
      color: white;
      background-color: white
        /*#4CAF50;  Green */
        border: none;
      color: black;
      padding: 15px 32px;
      text-align: center;
      text-decoration: none;
      display: inline-block;
    }

    #start {
      background-color: green
    }

    #startconsoledebug {
      background-color: yellow
    }

    #stopconsoledebug {
      background-color: orange
    }

    #microphonestatus,
    #audiostatus,
    #recording {
      font-size: 36px;
      font-family: 'courier';
      background-color: black;
      color: white;
    }

    td,
    th {
      border: 1px solid;
      text-align: left;
      padding: 10px;
    }
  </style>
</head>

<body>
  <h1>WeBAD Demo</h1>
  <h2>Web Browser Audio Detection/Speech Recording Events API</h2>
  <div>
    This sample shows how to implement an audio/speech detection in the browser, using a ScriptProcessor (Web Audio
    API).<br>
    Github repo: <a href="https://github.com/solyarisoftware/webad">https://github.com/solyarisoftware/webad</a><br>
    <br>
    To open/close the console: CTRL-SHIFT-J on Chrome, or CTRL-SHIFT-K on Firefox.
    <h1 id="transcript">SpeechRecognition:</h1>
  </div>
  <br>
  <button id="start">START (required on Chrome)</button>
  <button id="startconsoledebug">enable console logs</button>
  <button id="stopconsoledebug">disable console logs</button>
  <br>
  <br>
  <div>
    REAL-TIME EVENTS/STATUS
    <table>
      <tr>
        <th>Status</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>microphone</td>
        <td id="microphonestatuscell"><a id="microphonestatus"></a></td>
      </tr>
      <tr>
        <td>audio</td>
        <td id="audiostatuscell"><a id="audiostatus"></a></td>
      </tr>
      <tr>
        <td>speech<br>recording</td>
        <td id="recordingcell"><a id="recording"></a>
          <p><audio id="audio" controls></audio></p>
        </td>
      </tr>
    </table>
  </div>

  <br>

  <div>
    CONFIGURATION PARAMETERS
    <table>
      <tr>
        <th>Parameter</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>SAMPLE_POLLING_MSECS</td>
        <td><a id="SAMPLE_POLLING_MSECS"></a></td>
      </tr>
      <tr>
        <td>MAX_INTERSPEECH_SILENCE_MSECS</td>
        <td><a id="MAX_INTERSPEECH_SILENCE_MSECS"></a></td>
      </tr>
      <tr>
        <td>MIN_SIGNAL_DURATION</td>
        <td><a id="MIN_SIGNAL_DURATION"></a></td>
      </tr>
      <tr>
        <td>VOLUME_SIGNAL</td>
        <td><a id="VOLUME_SIGNAL"></a></td>
      </tr>
      <tr>
        <td>VOLUME_SILENCE</td>
        <td><a id="VOLUME_SILENCE"></a></td>
      </tr>
      <tr>
        <td>VOLUME_MUTE</td>
        <td><a id="VOLUME_MUTE"></a></td>
      </tr>
      <tr>
        <td>MIN_AVERAGE_SIGNAL_VOLUME</td>
        <td><a id="MIN_AVERAGE_SIGNAL_VOLUME"></a></td>
      </tr>
    </table>
  </div>

  <br>

  <div>
    EVENTS EMITTED
    <table>
      <tr>
        <th>Event name</th>
        <th>Category</th>
        <th>Description</th>
      </tr>
      <tr>
        <td>signal</td>
        <td>audio sample</td>
        <td>volume is high, so probably user is speaking</td>
      </tr>
      <tr>
        <td>silence</td>
        <td>audio sample</td>
        <td>volume is pretty low, the mic is on but there is not speech</td>
      </tr>
      <tr>
        <td>mute</td>
        <td>audio sample</td>
        <td>volume is almost zero, the mic is off</td>
      </tr>
      <tr>
        <td>unmutedmic</td>
        <td>microphone</td>
        <td>micro is UNMUTED (passing from OFF to ON)</td>
      </tr>
      <tr>
        <td>mutedmic</td>
        <td>microphone</td>
        <td>micro is MUTED (passing from ON to OFF)</td>
      </tr>
      <tr>
        <td>prespeechstart</td>
        <td>recording</td>
        <td>speech recording start</td>
      </tr>
      <tr>
        <td>speechstart</td>
        <td>recording</td>
        <td>speech initial chunk start</td>
      </tr>
      <tr>
        <td>speechstop</td>
        <td>recording</td>
        <td>speech recording stop</td>
      </tr>
      <tr>
        <td>speechabort</td>
        <td>recording</td>
        <td>speech recording aborted</td>
      </tr>
    </table>
  </div>


  <script>/*
    The MIT License (MIT)
    
    Copyright (c) 2014 Chris Wilson
    
    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:
    
    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.
    
    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE.
    */
    
    /*
    
    Usage:
    audioNode = createAudioMeter(audioContext,clipLevel,averaging,clipLag);
    
    audioContext: the AudioContext you're using.
    clipLevel: the level (0 to 1) that you would consider "clipping".
       Defaults to 0.98.
    averaging: how "smoothed" you would like the meter to be over time.
       Should be between 0 and less than 1.  Defaults to 0.95.
    clipLag: how long you would like the "clipping" indicator to show
       after clipping has occured, in milliseconds.  Defaults to 750ms.
    
    Access the clipping through node.checkClipping(); use node.shutdown to get rid of it.
    */
    
    function createAudioMeter(audioContext,clipLevel,averaging,clipLag) {
      var processor = audioContext.createScriptProcessor(512);
      processor.onaudioprocess = volumeAudioProcess;
      processor.clipping = false;
      processor.lastClip = 0;
      processor.volume = 0;
      processor.clipLevel = clipLevel || 0.98;
      processor.averaging = averaging || 0.95;
      processor.clipLag = clipLag || 750;
    
      // this will have no effect, since we don't copy the input to the output,
      // but works around a current Chrome bug.
      processor.connect(audioContext.destination);
    
      processor.checkClipping =
        function(){
          if (!this.clipping)
            return false;
          if ((this.lastClip + this.clipLag) < window.performance.now())
            this.clipping = false;
          return this.clipping;
        };
    
      processor.shutdown =
        function(){
          this.disconnect();
          this.onaudioprocess = null;
        };
    
      return processor;
    }
    
    function volumeAudioProcess( event ) {
      var buf = event.inputBuffer.getChannelData(0);
        var bufLength = buf.length;
      var sum = 0;
        var x;
    
      // Do a root-mean-square on the samples: sum up the squares...
        for (var i=0; i<bufLength; i++) {
          x = buf[i];
          if (Math.abs(x)>=this.clipLevel) {
            this.clipping = true;
            this.lastClip = window.performance.now();
          }
          sum += x * x;
        }
    
        // ... then take the square root of the sum.
        var rms =  Math.sqrt(sum / bufLength);
    
        // Now smooth this out with the averaging factor applied
        // to the previous sample - take the max here because we
        // want "fast attack, slow release."
        this.volume = Math.max(rms, this.volume*this.averaging);
    }
    </script>



  <script>/**
    * audioDetectionConfig.js
    *
    * configuration parameters
    */
   
   /*
    
   SAMPLE_POLLING_MSECS
   
   polling time clock in milliseconds. Is the sampling rate to run speech detection calculations.
   
             █                   
         █   █   █               
         █   █   █   █         
         █   █   █   █   █        
     █   █   █   █   █   █   █    
     █   █   █   █   █   █   █   █    
     █   █   █   █   █   █   █   █   █    
     █   █   █   █   █   █   █   █   █   █   █ 
     █   █   █   █   █   █   █   █   █   █   █   █

   
   */

   const SAMPLE_POLLING_MSECS = 50
   
   /*
    
   MAX_INTERSPEECH_SILENCE_MSECS
   
   elapsed time in milliseconds of silence (pause) between continuous blocks of signal.
   This is to decide when to stop recording of a speech made by multiple audio chunks separated by pauses.
   
   That elapsed is used also to decide if a full speech is concluded, generating event 'stoprecording'.
   
         █  chunk 1                    █
       █ █ █       █             █     █ █ chunk 2      █
       █ █ █ █ █   █   █         █   █ █ █              █ chunk 3
       █ █ █ █ █ █ █ █ █       █ █   █ █ █              █ █   █
     █ █ █ █ █ █ █ █ █ █ █     █ █   █ █ █              █ █ █ █
     █ █ █ █ █ █ █ █ █ █ █     █ █ █ █ █ █ █            █ █ █ █ █ █
     █ █ █ █ █ █ █ █ █ █ █ █   █ █ █ █ █ █ █ █          █ █ █ █ █ █ █
     █ █ █ █ █ █ █ █ █ █ █ █   █ █ █ █ █ █ █ █          █ █ █ █ █ █ █ █
     █ █ █ █ █ █ █ █ █ █ █ █   █ █ █ █ █ █ █ █          █ █ █ █ █ █ █ █
                            <->               <-------->               <-------->
     ^                                         silence                   silence ^
     |                                            ^                         ^    |
     speechstart                                  |                         |    speechstop 
                                                  |                         |
                                    MAX_INTERSPEECH_SILENCE_MSECS    POST_SPEECH_MSECS
   */                                                      
   const MAX_INTERSPEECH_SILENCE_MSECS = 600
   const POST_SPEECH_MSECS = MAX_INTERSPEECH_SILENCE_MSECS
   
   
   /*
    
   PRERECORDSTART_MSECS
   
   elapsed time in milliseconds before the speechstart event.
   
   
                 █   chunk 1                     █
               █ █ █       █             █     █ █              █
               █ █ █ █ █   █   █         █   █ █ █ chunk 2      █
               █ █ █ █ █ █ █ █ █       █ █   █ █ █              █ █   █ chunk 3
             █ █ █ █ █ █ █ █ █ █ █     █ █   █ █ █              █ █ █ █
             █ █ █ █ █ █ █ █ █ █ █     █ █ █ █ █ █ █            █ █ █ █ █ █
             █ █ █ █ █ █ █ █ █ █ █ █   █ █ █ █ █ █ █ █          █ █ █ █ █ █ █
             █ █ █ █ █ █ █ █ █ █ █ █   █ █ █ █ █ █ █ █          █ █ █ █ █ █ █ █
             █ █ █ █ █ █ █ █ █ █ █ █   █ █ █ █ █ █ █ █          █ █ █ █ █ █ █ █
   <-------->                       <->               <-------->               <--------> 
   ^ silence ^                                                                           ^
   |    ^    |                                                                           |
   |    |    speechstart                                                        speechstop  
   |    |
   | PRERECORDSTART_MSECS             
   |              
   prerecordstart
   
   */                                                      
   const PRERECORDSTART_MSECS = 600
   
   /*
    
   MIN_SIGNAL_DURATION 
   
   minimum elapsed time in millisecond for an audio signal block.
   In terms of speech, it corresponds to a letter spelling ('b'), 
   a number splelling ('two'), a 'yes'/'no' speech.
   
   It could be usefule to purge out background clicks/noises.
   If a signal block chain sample length is less than that value, 
   the event 'abortrecording' is generated.
   
         █                                
       █ █ █       █ chunk 1     █      
       █ █ █ █ █   █   █         █ chunk 2     █
       █ █ █ █ █ █ █ █ █       █ █           █ █
     █ █ █ █ █ █ █ █ █ █ █     █ █           █ █ chunk 3
     █ █ █ █ █ █ █ █ █ █ █     █ █ █       █ █ █
     █ █ █ █ █ █ █ █ █ █ █ █   █ █ █       █ █ █
     █ █ █ █ █ █ █ █ █ █ █ █   █ █ █       █ █ █
     █ █ █ █ █ █ █ █ █ █ █ █   █ █ █       █ █ █
                               <--->       <--->   
                                 ^
                                 |
                          MIN_SIGNAL_DURATION  
   */
   const MIN_SIGNAL_DURATION = 50
   
   
   /*
    
   VOLUME_MUTE
   VOLUME_SILENCE
   VOLUME_SIGNAL
   
   Volume Thresholds levels, for 
   
   signal (speech)
   silence (background noise)
   mute (microphone off)
   
   */
   const VOLUME_SIGNAL = 0.02
   const VOLUME_SILENCE = 0.001
   const VOLUME_MUTE = 0.0001
   
   /*
    
   MIN_AVERAGE_SIGNAL_VOLUME
   Minimum volume vale (in average) of a signal block chain.
   It is to calculate if a signal block contains speech or just noise.
   
   If a signal block chain sample doesn't exceed that threshold value, 
   the event 'abortrecording' is generated.
   
   */
   const MIN_AVERAGE_SIGNAL_VOLUME = 0.01
   
   
   const DEFAULT_PARAMETERS_CONFIGURATION = {
   
     timeoutMsecs: SAMPLE_POLLING_MSECS,
     
     prespeechstartMsecs: PRERECORDSTART_MSECS,
     
     speakingMinVolume: VOLUME_SIGNAL, 
     
     silenceVolume: VOLUME_SILENCE,
     
     muteVolume: VOLUME_MUTE,
   
     recordingEnabled: true
   
   }
   
   </script>



  <script>/* eslint-env browser */

/**
 *
 * @see
 * https://stackoverflow.com/questions/9018771/how-to-best-determine-volume-of-a-signal
 * https://dsp.stackexchange.com/questions/46147/how-to-get-the-volume-level-from-pcm-audio-data
 *
 */ 


/**
 * volumeState
 *
 * volume range state of a single sample. Possible values:
 *
 *   'mute'
 *   'silence'
 *   'signal'
 *   'clipping' TODO
 *
 */ 
let volumeState = 'mute'

let speechStarted = false

let silenceItems = 0
let signalItems = 0

let speechstartTime 
let prerecordingItems = 0

let speechVolumesList = [] 

/**
 * functions
 */

/*
 * average
 *
 * calculate the average value of an array of numbers
 *
 */ 
const average = (array) => array.reduce((a, b) => a + b) / array.length

const averageSignal = () => average(speechVolumesList).toFixed(4)

const maxSilenceItems = Math.round(MAX_INTERSPEECH_SILENCE_MSECS / SAMPLE_POLLING_MSECS)

const dispatchEvent = (eventName, eventData) => document.dispatchEvent(new CustomEvent( eventName, eventData ))

/**
 * mute
 *
 * Emits 2 custom events:
 *
 *  AUDIO SAMPLING:
 *    'mute'    -> audio volume is almost zero, the mic is off.
 *
 *  MICROPHONE:
 *    'mutedmic' -> microphone is MUTED (passing from ON to OFF)
 */
function mute(timestamp, duration) {

  const eventData = { 
    detail: { 
      event: 'mute',
      volume: meter.volume, 
      timestamp,
      duration
    } 
  }
  
  dispatchEvent( 'mute', eventData )
  
  // mic is muted (is closed)
  // trigger event on transition
  if (volumeState !== 'mute') {
    dispatchEvent( 'mutedmic', eventData )
    volumeState = 'mute'
  }  

}  


/**
 * signal
 *
 * Emits 3 custom events:
 *
 *  AUDIO SAMPLING:
 *    'signal'  -> audio volume is high, so probably user is speaking.
 *
 *  MICROPHONE:
 *    'unmutedmic'  -> microphone is UNMUTED (passing from OFF to ON)
 *
 *  RECORDING:
 *    'speechstart' -> speech START
 *
 */ 
function signal(timestamp, duration) {

  silenceItems = 0
  
  const eventData = { 
    detail: { 
      event: 'signal',
      volume: meter.volume, 
      timestamp,
      duration,
      items: ++ signalItems
    } 
  }
 
  if (! speechStarted) {

    dispatchEvent( 'speechstart', eventData )

    speechstartTime = timestamp
    speechStarted = true
    speechVolumesList = []
  }  

  speechVolumesList.push(meter.volume)

  dispatchEvent( 'signal', eventData )

  // mic is unmuted (is open)
  // trigger event on transition
  if (volumeState === 'mute') {
    dispatchEvent( 'unmutedmic', eventData )
    volumeState = 'signal'
  }  

}  

/**
 * silence
 *
 * Emits 3 custom events:
 *
 *  AUDIO SAMPLING:
 *    'silence' -> audio volume is pretty low, the mic is on but there is not speech.
 *
 *  MICROPHONE:
 *    'unmutedmic'  -> microphone is UNMUTED (passing from OFF to ON)
 *
 *  RECORDING:
 *    'speechstop'  -> speech recording STOP (success, recording seems a valid speech)
 *    'speechabort' -> speech recording ABORTED (because level is too low or audio duration length too short)
 *
 */ 
function silence(timestamp, duration) {

  signalItems = 0

  const eventData = { 
    detail: { 
      event: 'silence',
      volume: meter.volume, 
      timestamp,
      duration,
      items: ++ silenceItems
    } 
  }
 
  dispatchEvent( 'silence', eventData )

  // mic is unmuted (goes ON)
  // trigger event on transition
  if (volumeState === 'mute') {
    dispatchEvent( 'unmutedmic', eventData )
    volumeState = 'silence'
  }  

  //
  // after a MAX_INTERSPEECH_SILENCE_MSECS 
  // a virdict event is generated:
  //   speechabort if audio chunck is to brief or at too low volume 
  //   speechstop  if audio chunk appears to be a valid speech
  //
  if ( speechStarted && (silenceItems === maxSilenceItems) ) {

    const signalDuration = duration - MAX_INTERSPEECH_SILENCE_MSECS
    const averageSignalValue = averageSignal()

    // speech abort 
    // signal duration too short
    if ( signalDuration < MIN_SIGNAL_DURATION ) {

      eventData.detail.abort = `signal duration (${signalDuration}) < MIN_SIGNAL_DURATION (${MIN_SIGNAL_DURATION})`
      dispatchEvent( 'speechabort', eventData )
    }  

    // speech abort
    // signal level too low
    else if (averageSignalValue < MIN_AVERAGE_SIGNAL_VOLUME) {

      eventData.detail.abort = `signal average volume (${averageSignalValue}) < MIN_AVERAGE_SIGNAL_VOLUME (${MIN_AVERAGE_SIGNAL_VOLUME})`
      dispatchEvent( 'speechabort', eventData )
    }  

    // speech stop
    // audio chunk appears to be a valid speech
    else {


      dispatchEvent( 'speechstop', eventData )
    }  

    speechStarted = false
  }  

}  

/**
    volume level
               
*/ 

function sampleThresholdsDecision(muteVolume, speakingMinVolume) {

  const timestamp = Date.now()
  const duration = timestamp - speechstartTime

  //
  // MUTE
  // mic is OFF/mute (volume is ~0)
  //
  if (meter.volume < muteVolume )

    mute(timestamp, duration) 

  //
  // SIGNAL
  // audio detection, maybe it's SPEECH
  //
  else if (meter.volume > speakingMinVolume )

    signal(timestamp, duration)

  //
  // SILENCE
  // mic is ON. Audio level is low (background noise)
  //
  else //(meter.volume < config.silenceVolume )

    silence(timestamp, duration)

}


/**
 * prerecording
 *
 * Emits the event:
 *
 *  RECORDING:
 *    'prespeechstart' -> speech prerecording START
 *
 * Every prespeechstartMsecs milliseconds, 
 * in SYNC with the main sampling (every timeoutMsecs milliseconds)
 *
 * @param {Number} prespeechstartMsecs
 * @param {Number} timeoutMsecs
 *
 */ 
function prerecording( prespeechstartMsecs, timeoutMsecs ) {
  
  ++ prerecordingItems

  const eventData = { 
    detail: { 
      //event: 'prespeechstart',
      volume: meter.volume, 
      timestamp: Date.now(),
      items: prerecordingItems
    } 
  }

  // emit event 'prespeechstart' every prespeechstartMsecs.
  // considering that prespeechstartMsecs is a multimple of timeoutMsecs   
  if ( (prerecordingItems * timeoutMsecs) >= prespeechstartMsecs) {
    
    // emit the event if speech is not started   
    if ( !speechStarted )
      dispatchEvent( 'prespeechstart', eventData )

    prerecordingItems = 0
  }  

}  


/**
 * audio speech detection
 *
 * emit these DOM custom events: 
 *
 *  AUDIO SAMPLING:
 *    'clipping' -> TODO, audio volume is clipping (~1), 
 *                  probably user is speaking, but volume produces distorsion
 *    'signal'   -> audio volume is high, so probably user is speaking.
 *    'silence'  -> audio volume is pretty low, the mic is on but there is not speech.
 *    'mute'     -> audio volume is almost zero, the mic is off.
 *
 *  MICROPHONE:
 *    'unmutedmic'  -> microphone is UNMUTED (passing from OFF to ON)
 *    'mutedmic'    -> microphone is MUTED (passing from ON to OFF)
 *
 *  RECORDING:
 *    'prespeechstart' -> speech prerecording START
 *    'speechstart'    -> speech START
 *    'speechstop'     -> speech STOP (success, recording seems a valid speech)
 *    'speechabort'    -> speech ABORTED (because level is too low or audio duration length too short)
 *
 *
 * @param {Object} config 
 * @see DEFAULT_PARAMETERS_CONFIGURATION object in audioDetectionConfig.js 
 *
 * @see https://javascript.info/dispatch-events
 *
 */

function audioDetection(config) {

  setTimeout( 
    () => {

      prerecording( config.prespeechstartMsecs, config.timeoutMsecs )

      // to avoid feedback, recording could be suspended 
      // when the system play audio with a loudspeakers
      if (config.recordingEnabled) {

        sampleThresholdsDecision(config.muteVolume, config.speakingMinVolume)
      }  

      // recursively call this function
      audioDetection(config)

    }, 
    config.timeoutMsecs 
  )

}


//export { audioDetection }

</script>



  <script>
    var mediaStreamSource = null
    
    
    function audioStream(stream) {
      // Create an AudioNode from the stream.
      mediaStreamSource = audioContext.createMediaStreamSource(stream);
    
      // Create a new volume meter and connect it.
      meter = createAudioMeter(audioContext);
      mediaStreamSource.connect(meter);
    
      // kick off the visual updating
      //drawLoop();
    
      audioDetection(DEFAULT_PARAMETERS_CONFIGURATION)
    
      audioRecorder(stream)
    }
    
    </script>



  <script>/* eslint-env browser */
    var recorder = null
    
    let audioPlay = true
    
    // This example uses MediaRecorder to record from a live audio stream,
    // and uses the resulting blob as a source for an audio element.
    //
    // The relevant functions in use are:
    //
    // navigator.mediaDevices.getUserMedia -> to get audio stream from microphone
    // MediaRecorder (constructor) -> create MediaRecorder instance for a stream
    // MediaRecorder.ondataavailable -> event to listen to when the recording is ready
    // MediaRecorder.start -> start recording
    // MediaRecorder.stop -> stop recording (this will generate a blob of data)
    // URL.createObjectURL -> to create a URL from a blob, which we can use as audio src
    
    
    function audioRecorder(stream) {
    
      recorder = new MediaRecorder(stream)
    
      // listen to dataavailable, 
      // which gets triggered whenever we have
      // an audio blob available
      recorder.addEventListener('dataavailable', onRecordingReady)
    
    }
    
    
    function onRecordingReady(e) {
    
      //
      // listen recording (audio play) 
      // just if speech is not aborted
      //
      if (audioPlay) {
    
        //
        // you don't want to record while playing (through loudspeakers), 
        // to avoid that playback audio feedback in the mic input!
        // 
        suspendRecording()
    
        document.querySelector('#audiostatuscell').style.background = 'orange'
        document.querySelector('#audiostatuscell').style.color = 'black'
        document.querySelector('#audiostatus').style.background = 'orange'
        document.querySelector('#audiostatus').textContent = 'playback'
    
        const audio = document.getElementById('audio')
    
        // e.data contains a blob representing the recording
        audio.src = URL.createObjectURL(e.data)
    
        audio.play()
        
        //
        // you want to resume recording after the audio playback
        //
        audio.onended = () => {
          resumeRecording() 
          //console.log('recordingEnabled ' + DEFAULT_PARAMETERS_CONFIGURATION.recordingEnabled)
        }  
      }
      
    }
    
    
    function startRecording() {
      recorder.start()
    }
    
    
    function stopRecording() {
      // Stopping the recorder will eventually trigger the `dataavailable` event and we can complete the recording process
      recorder.stop()
      audioPlay = true
    }
    
    
    
    /**
     * restartRecording
     *
     * abort and start
     */ 
    function restartRecording() {
      
      // https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder/state
      //console.log('recorder ' +  recorder.state )
    
      // need otherwise I get on Chrome the error:
      // Failed to execute 'stop' on 'MediaRecorder': The MediaRecorder's state is 'inactive'.
      if (recorder.state != 'inactive')
        recorder.stop()
    
      audioPlay = false
      recorder.start()
      
    }
    
    function abortRecording() {
      // Stopping the recorder will eventually trigger the `dataavailable` event and we can complete the recording process
      recorder.stop()
      audioPlay = false
    }
    
    
    // to suspend recording when the system play audio with a loudspeaker, avoiding feedback
    function suspendRecording() {
      DEFAULT_PARAMETERS_CONFIGURATION.recordingEnabled = false
    }  
    
    
    function resumeRecording() {
      DEFAULT_PARAMETERS_CONFIGURATION.recordingEnabled = true
    }  
    
    
    </script>



  <script>/* eslint-env browser */

    const dB = (signal) => - Math.round( 20 * Math.log10( 1 / signal ) ) 
    
    /**
     * speechDetectionListeners
     *
     */ 
    
    function hystogramLine( value ) {
    
      const maxCharsperLine = 200
      const valueInChars = maxCharsperLine * value 
      const char = '█'
    
      return char.repeat(valueInChars) 
    
    }  
    
    
    function showConfiguration() {
    
      document.querySelector('#SAMPLE_POLLING_MSECS').textContent = SAMPLE_POLLING_MSECS
      document.querySelector('#MAX_INTERSPEECH_SILENCE_MSECS').textContent = MAX_INTERSPEECH_SILENCE_MSECS
      document.querySelector('#MIN_SIGNAL_DURATION').textContent = MIN_SIGNAL_DURATION
      document.querySelector('#VOLUME_SIGNAL').textContent = VOLUME_SIGNAL
      document.querySelector('#VOLUME_SILENCE').textContent = VOLUME_SILENCE
      document.querySelector('#VOLUME_MUTE').textContent = VOLUME_MUTE
      document.querySelector('#MIN_AVERAGE_SIGNAL_VOLUME').textContent = MIN_AVERAGE_SIGNAL_VOLUME
    
    }  
    
    
    //
    // signal handler
    //
    document.addEventListener('signal', event => {
    
      const volume = event.detail.volume.toFixed(9)
      const timestamp = event.detail.timestamp
      const items = event.detail.items.toString().padEnd(3)
      const dBV = dB(event.detail.volume)
    
      const line = hystogramLine(volume)
    
      if (debuglog)
        console.log(`signal  ${timestamp} ${items} ${volume} ${dBV} ${line}`)
    
      document.querySelector('#audiostatuscell').style.background = 'green'
      document.querySelector('#audiostatuscell').style.color = 'black'
      document.querySelector('#audiostatus').style.background = 'green'
      document.querySelector('#audiostatus').textContent = 'signal'
    
      //const theDiv = document.getElementById('log')
      //const content = document.createTextNode(text)
      //theDiv.appendChild(content)
    
    })
    
    //
    // silence handler
    //
    document.addEventListener('silence', event => {
    
      const volume = event.detail.volume.toFixed(9)
      const timestamp = event.detail.timestamp
      const items = event.detail.items.toString().padEnd(3)
      const dBV = dB(event.detail.volume)
    
      if (debuglog)
        console.log(`silence ${timestamp} ${items} ${volume} ${dBV}`)
    
      document.querySelector('#audiostatuscell').style.background = 'black'
      document.querySelector('#audiostatuscell').style.color = 'white'
      document.querySelector('#audiostatus').style.background = 'black'
      document.querySelector('#audiostatus').textContent = 'silence'
    
    })
    
    //
    // mute handler
    //
    document.addEventListener('mute', event => {
    
      const volume = event.detail.volume.toFixed(9)
      const timestamp = event.detail.timestamp
      const dBV = dB(event.detail.volume)
    
      if (debuglog)
        console.log(`mute    ${timestamp} ${volume} ${dBV}`)
    
      document.querySelector('#audiostatus').textContent = 'mute'
    
    })
    
    
    //
    // prespeechstart handler
    //
    document.addEventListener('prespeechstart', event => {
    
      if (debuglog) {
        
        //const volume = event.detail.volume.toFixed(9)
        const timestamp = event.detail.timestamp
        //const dBV = dB(event.detail.volume)
    
        //console.log(`%cPRE SPEECH START    ${timestamp} ${volume} ${dBV}`, 'color:yellow')
        console.log(`%cPRE SPEECH START   ${timestamp}`, 'color:blue')
    
      }  
      
      restartRecording()
    
    })
    
    
    //
    // speechstart handler
    //
    document.addEventListener('speechstart', event => {
    
      if (debuglog) {
      
        //speechstartTime = event.detail.timestamp
        console.log('%cSPEECH START', 'color:greenyellow')
      }  
    
      document.querySelector('#recordingcell').style.background = 'green'
      document.querySelector('#recordingcell').style.color = 'white'
      document.querySelector('#recording').style.background = 'green'
      document.querySelector('#recording').style.color = 'white'
      document.querySelector('#recording').textContent = 'start'
    
      //startRecording()
    
    })
    
    //
    // speechstop handler
    //
    document.addEventListener('speechstop', event => {
    
      const duration = event.detail.duration
    
      // if (debuglog) {
        
        const averageSignalLevel = averageSignal()
        
        console.log('%cSPEECH STOP', 'color:lime')
        console.log(`Total Duration in msecs  : ${duration}`)
        console.log(`Signal Duration in msecs : ${duration - MAX_INTERSPEECH_SILENCE_MSECS }`)
        console.log(`Average Signal level     : ${averageSignalLevel}`)
        console.log(`Average Signal dB        : ${dB(averageSignalLevel)}`)
        console.log(' ')
      // }  
    
      document.querySelector('#recordingcell').style.color = 'white'
      document.querySelector('#recordingcell').style.background = 'black'
      document.querySelector('#recording').style.color = 'white'
      document.querySelector('#recording').style.background = 'black'
      document.querySelector('#recording').textContent = `stop. len: ${duration} msecs`
    
    
      stopRecording()
    
    })
    
    //
    // speechabort handler
    //
    document.addEventListener('speechabort', event => {
    
      const abort = event.detail.abort
    
      if (debuglog) {
        
        const duration = event.detail.duration
        const averageSignalLevel = averageSignal()
        
        console.log('%cSPEECH ABORT', 'color:red')
        console.log(`Abort reason             : ${abort}`)
        console.log(`Total Duration in msecs  : ${duration}`)
        console.log(`Signal Duration in msecs : ${duration - MAX_INTERSPEECH_SILENCE_MSECS }`)
        console.log(`Average Signal level     : ${averageSignalLevel}`)
        console.log(`Average Signal dB        : ${dB(averageSignalLevel)}`)
        console.log(' ')
      }  
    
      document.querySelector('#recordingcell').style.color = 'white'
      document.querySelector('#recordingcell').style.background = 'red'
      document.querySelector('#recording').style.color = 'white'
      document.querySelector('#recording').style.background = 'red'
      document.querySelector('#recording').textContent = `abort. ${abort}`
    
      abortRecording()
    
    })
    
    //
    // mutedmic handler
    //
    document.addEventListener('mutedmic', event => {
    
      document.querySelector('#microphonestatus').textContent = 'muted (off)'
      document.querySelector('#microphonestatus').style.background = 'red'
      document.querySelector('#microphonestatuscell').style.background = 'red'
    
      console.log('%cMICROPHONE MUTED', 'color:red')
      console.log(' ')
    
    })
    
    //
    // unmutedmic handler
    //
    document.addEventListener('unmutedmic', event => {
    
      document.querySelector('#microphonestatus').textContent = 'unmuted (on)'
      document.querySelector('#microphonestatus').style.background = 'green'
      document.querySelector('#microphonestatuscell').style.background = 'green'
    
      console.log('%cMICROPHONE UNMUTED', 'color:green')
      console.log(' ')
    
    })
    
    
    showConfiguration()
    
    </script>
  
  
  
  <script>/* eslint-env browser */
    /*
    The MIT License (MIT)
    
    Copyright (c) 2014 Chris Wilson
    
    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:
    
    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.
    
    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE.
    */
    var audioContext = null;
    var meter = null;
    var canvasContext = null;
    var WIDTH=500;
    var HEIGHT=50;
    var rafID = null;
    
    var debuglog = false
    
    window.onload = function() {
    
        // grab our canvas
        //canvasContext = document.getElementById( 'meter' ).getContext('2d');
      
        // monkeypatch Web Audio
        window.AudioContext = window.AudioContext || window.webkitAudioContext;
      
        // grab an audio context
        audioContext = new AudioContext();
    
        // https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio
        // https://sites.google.com/a/chromium.org/dev/audio-video/autoplay
        // https://stackoverflow.com/questions/50218162/web-autoplay-policy-change-resuming-context-doesnt-unmute-audio
      
        // One-liner to resume playback when user interacted with the page.
        document.querySelector('#start').addEventListener('click', function() {
    
          audioContext.resume().then( () => {
            console.log('User interacted with the page. Playback resumed successfully')
          })
    
        })
    
        document.querySelector('#startconsoledebug').addEventListener('click', function() {
          debuglog = true
        })
    
        // debug log flag
        document.querySelector('#stopconsoledebug').addEventListener( 'click', () =>  { 
          debuglog = false 
        })
    
        // Attempt to get audio input
        try {
            // ask for an audio input
            navigator.mediaDevices.getUserMedia(
            {
                'audio': {
                    'mandatory': {
                        'googEchoCancellation': 'false',
                        'googAutoGainControl': 'false',
                        'googNoiseSuppression': 'false',
                        'googHighpassFilter': 'false'
                    },
                    'optional': []
                },
            }).then(audioStream)
            .catch(didntGetStream);
        } catch (e) {
            alert('getUserMedia threw exception :' + e);
        }
    
    }
    
    
    function didntGetStream() {
        alert('Stream generation failed.');
    }
    
    
    function drawLoop( time ) {
        // clear the background
        canvasContext.clearRect(0,0,WIDTH,HEIGHT);
    
        // check if we're currently clipping
        if (meter.checkClipping())
            canvasContext.fillStyle = 'red';
        else
            canvasContext.fillStyle = 'green';
    
        // draw a bar based on the current volume
        canvasContext.fillRect(0, 0, meter.volume*WIDTH*1.4, HEIGHT);
    
        // set up the next visual callback
        rafID = window.requestAnimationFrame( drawLoop );
    }
    
    
    </script>



  <script>
    const transcriptElement = document.getElementById('transcript');
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (SpeechRecognition) {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-us';

      let result = [];
      recognition.onresult = event => {
        console.log('Speech recognition results:', event.results);
        const len = event.results.length;

        if (event.results[len - 1].isFinal) {
          // 如果是最终结果，只显示最后的转录内容
          if (event.results[len - 1][0].transcript) {
            const latestTranscript = event.results[len - 1][0].transcript;
            transcriptElement.textContent = 'SpeechRecognition: ' + latestTranscript;
          }
        } else {
          // 如果是中间结果，显示当前的转录内容
          const data = event.results[len - 1][0].transcript;
          transcriptElement.textContent = 'SpeechRecognition: ' + data;
        }
      };

      recognition.start();
    } else {
      console.log("Speech recognition not supported in this browser.");
    }
  </script>
</body>

</html>
